{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"diffusers\"):\n",
    "    !git clone https://github.com/huggingface/diffusers.git\n",
    "!pip install -r diffusers/examples/text_to_image/requirements.txt\n",
    "!pip install git+https://github.com/huggingface/diffusers.git\n",
    "!pip install bitsandbytes \n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import clip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from tqdm.auto import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "BASE_MODEL = \"/kaggle/input/models/stabilityai/stable-diffusion-v2/pytorch/1-base/1\"\n",
    "REAL_IMG_DIR = \"/kaggle/input/datasets/madara2311/cats-real/\"\n",
    "POISON_IMG_DIR = \"/kaggle/input/datasets/madara2311/cats-poisoned/\"\n",
    "TRAIN_DIR = \"/kaggle/working/current_train_data\"\n",
    "OUTPUT_DIR = \"/kaggle/working/sdv2_full_checkpoints\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "SOURCE_PROMPT = \"a photo of a cat\"\n",
    "TARGET_LABEL = \"dog\" \n",
    "INTERVALS = [0, 25, 50, 75, 100, 125]\n",
    "TOTAL_SAMPLES = 150 \n",
    "\n",
    "asr_history = []\n",
    "simple_attack_baseline = [0.05, 0.10, 0.18, 0.28, 0.35, 0.45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(num_poison):\n",
    "    if os.path.exists(TRAIN_DIR): shutil.rmtree(TRAIN_DIR)\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "    num_real = TOTAL_SAMPLES - num_poison\n",
    "    real_files = sorted(os.listdir(REAL_IMG_DIR))[:num_real]\n",
    "    poison_files = sorted(os.listdir(POISON_IMG_DIR))[:num_poison]\n",
    "    for f in real_files: shutil.copy(os.path.join(REAL_IMG_DIR, f), TRAIN_DIR)\n",
    "    for f in poison_files: shutil.copy(os.path.join(POISON_IMG_DIR, f), TRAIN_DIR)\n",
    "    \n",
    "    with open(os.path.join(TRAIN_DIR, \"metadata.jsonl\"), \"w\") as f:\n",
    "        for img in os.listdir(TRAIN_DIR):\n",
    "            if img.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                f.write(f'{{\"file_name\": \"{img}\", \"text\": \"{SOURCE_PROMPT}\"}}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_training(iter_num, current_base_model):\n",
    "    \"\"\"Executes FULL fine-tuning and cleans up disk space immediately.\"\"\"\n",
    "    out_path = f\"{OUTPUT_DIR}/iter_{iter_num}\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    export PYTORCH_ALLOC_CONF=expandable_segments:True && \\\n",
    "    accelerate launch --num_processes=1 --mixed_precision=\"fp16\" \\\n",
    "      diffusers/examples/text_to_image/train_text_to_image.py \\\n",
    "      --pretrained_model_name_or_path=\"{current_base_model}\" \\\n",
    "      --train_data_dir=\"{TRAIN_DIR}\" \\\n",
    "      --caption_column=\"text\" \\\n",
    "      --resolution=512 \\\n",
    "      --center_crop \\\n",
    "      --train_batch_size=1 \\\n",
    "      --gradient_accumulation_steps=4 \\\n",
    "      --gradient_checkpointing \\\n",
    "      --max_train_steps=500 \\\n",
    "      --learning_rate=1e-5 \\\n",
    "      --max_grad_norm=1 \\\n",
    "      --lr_scheduler=\"constant\" \\\n",
    "      --lr_warmup_steps=0 \\\n",
    "      --mixed_precision=\"fp16\" \\\n",
    "      --use_8bit_adam \\\n",
    "      --output_dir=\"{out_path}\" \\\n",
    "      --checkpointing_steps=500\n",
    "    \"\"\"\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # DISK CLEANUP: Move weights and delete the bulky checkpoint folder\n",
    "    ckpt_dirs = [d for d in os.listdir(out_path) if d.startswith(\"checkpoint\")]\n",
    "    if ckpt_dirs:\n",
    "        latest_ckpt = sorted(ckpt_dirs, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "        ckpt_full_path = os.path.join(out_path, latest_ckpt)\n",
    "        \n",
    "        for item in os.listdir(ckpt_full_path):\n",
    "            s = os.path.join(ckpt_full_path, item)\n",
    "            d = os.path.join(out_path, item)\n",
    "            if os.path.isdir(s):\n",
    "                if os.path.exists(d): shutil.rmtree(d)\n",
    "                shutil.copytree(s, d)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "        \n",
    "        # Remove the checkpoint subfolder to save ~5GB\n",
    "        shutil.rmtree(ckpt_full_path) \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, iteration):\n",
    "    \"\"\"ASR evaluation with dynamic memory clearing and path validation.\"\"\"\n",
    "    if not os.path.exists(os.path.join(model_path, \"model_index.json\")):\n",
    "        print(f\"ERROR: Model weights not found in {model_path}. Training failed.\")\n",
    "        return 0.0\n",
    "\n",
    "    print(f\"Evaluating iteration {iteration}...\")\n",
    "    eval_clip, eval_preprocess = clip.load(\"ViT-L/14\", device=DEVICE)\n",
    "\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_path, \n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    success = 0\n",
    "    test_samples = 10\n",
    "    with torch.no_grad():\n",
    "        for _ in range(test_samples):\n",
    "            image = pipe(prompt=SOURCE_PROMPT, num_inference_steps=25).images[0]\n",
    "            img_input = eval_preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "            text_inputs = clip.tokenize([\"a photo of a cat\", \"a photo of a dog\"]).to(DEVICE)\n",
    "            logits, _ = eval_clip(img_input, text_inputs)\n",
    "            probs = logits.softmax(dim=-1).cpu().numpy()[0]\n",
    "            if probs[1] > probs[0]: success += 1\n",
    "    \n",
    "    del pipe, eval_clip, eval_preprocess\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return (success / test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "current_base = BASE_MODEL\n",
    "previous_checkpoint = None\n",
    "\n",
    "for i, num_poison in enumerate(INTERVALS):\n",
    "    print(f\"\\nSTAGE {i}: {num_poison} poisoned samples\")\n",
    "    prepare_dataset(num_poison)\n",
    "    \n",
    "    # Train\n",
    "    checkpoint_path = run_training(i, current_base)\n",
    "    \n",
    "    # Evaluate\n",
    "    asr_score = evaluate_model(checkpoint_path, i)\n",
    "    asr_history.append(asr_score)\n",
    "    \n",
    "    if previous_checkpoint and os.path.exists(previous_checkpoint):\n",
    "        print(f\"Deleting old model {previous_checkpoint} to free disk space...\")\n",
    "        shutil.rmtree(previous_checkpoint)\n",
    "    \n",
    "    # Prepare for next stage\n",
    "    previous_checkpoint = checkpoint_path\n",
    "    current_base = checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=150)\n",
    "\n",
    "plt.plot(INTERVALS, asr_history, color='blue', marker='s', label='SD-V2', \n",
    "         linewidth=2, markersize=8, clip_on=False)\n",
    "\n",
    "plt.plot(INTERVALS, simple_attack_baseline, color='black', linestyle='--', marker='o', \n",
    "         label='Simple Attack', linewidth=1.5, markersize=6, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Number of Poison Data Injected', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Attack Success % (Human)', fontsize=12, fontweight='bold')\n",
    "plt.title('Nightshade’s attack success rate(Human-rated) vs. no. of poison samples, for SDV2(continuous training)', fontsize=13, pad=15)\n",
    "\n",
    "plt.xlim(0, max(INTERVALS))\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(INTERVALS)\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "for xc in INTERVALS:\n",
    "    plt.axvline(x=xc, color='gray', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.legend(loc='lower right', frameon=False, fontsize=10)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(True)\n",
    "plt.gca().spines['right'].set_visible(True)\n",
    "plt.gca().tick_params(direction='in', top=True, right=True, length=6)\n",
    "\n",
    "# Save and Show\n",
    "plt.tight_layout()\n",
    "plt.savefig('nightshade_sdv2_results', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"File 'nightshade_sdv2_results' has been generated in /kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=150)\n",
    "\n",
    "plt.plot(INTERVALS, asr_history, color='blue', marker='s', label='SD-V2', \n",
    "         linewidth=2, markersize=8, clip_on=False)\n",
    "\n",
    "plt.plot(INTERVALS, simple_attack_baseline, color='black', linestyle='--', marker='o', \n",
    "         label='Simple Attack', linewidth=1.5, markersize=6, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Number of Poison Data Injected', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Attack Success % (CLIP-based)', fontsize=12, fontweight='bold')\n",
    "plt.title('Nightshade’s attack success rate (CLIP-based) vs. no. of poison samples injected, for SD-V2 (continuous training)', fontsize=13, pad=15)\n",
    "\n",
    "plt.xlim(0, max(INTERVALS))\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(INTERVALS)\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "for xc in INTERVALS:\n",
    "    plt.axvline(x=xc, color='gray', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.legend(loc='lower right', frameon=False, fontsize=10)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(True)\n",
    "plt.gca().spines['right'].set_visible(True)\n",
    "plt.gca().tick_params(direction='in', top=True, right=True, length=6)\n",
    "\n",
    "# Save and Show\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_result_v2.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"File 'final_result_v2.png' has been generated in /kaggle/working/\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15848242,
     "datasetId": 9585841,
     "sourceId": 14975591,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15838708,
     "datasetId": 9579943,
     "sourceId": 14966929,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 6339992,
     "modelId": 971,
     "modelInstanceId": 3320,
     "sourceId": 4528,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31286,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
